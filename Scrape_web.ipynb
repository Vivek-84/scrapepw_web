{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8e8526",
   "metadata": {},
   "source": [
    "# What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c2e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping:\n",
    "# Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in\n",
    "# an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various\n",
    "# applications.\n",
    "#              The uses of Web Scraping for business as well as personal requirements are endless. Each business or individual\n",
    "# has their own specific need for gathering data.Some of the most common usage scenarios.\n",
    "\n",
    "# Lead Generation for Marketing:\n",
    "# A web scraping software can be used to generate leads for marketing. Email and Phone lists for cold outreach can be built by\n",
    "# scraping the data from relevant websites. For example, business contact details like phone number and email address can be\n",
    "# scraped from yellow pages websites or from Google Maps business listings.\n",
    "\n",
    "# Price Comparison & Competition Monitoring:\n",
    "# Companies catering products or services need to have comprehensive data of competitor products and services which appear in\n",
    "# the market every day. A web scraping software can be used to keep a constant watch on this data.\n",
    "\n",
    "# E-Commerce:\n",
    "# Web Scraping can be used to periodically extract data of products from various e-commerce websites like Amazon, eBay, Google\n",
    "# Shopping etc. Product details like price, description, images, reviews, rating etc. can be easily extracted using a web\n",
    "# scraping software.\n",
    "\n",
    "# Real Estate:\n",
    "# Property details displayed by real estate websites like Zillow, Realtor etc. can be extracted using a Web Scraping software.\n",
    "# In addition to scraping property details, web scraping can also be used to scrape agent and owner contact details.\n",
    "\n",
    "# Data Analysis:\n",
    "# You might want to collect and analyze data related to a specific category from multiple websites. The category might be real\n",
    "# estate, automobiles, electronic gadgets, industrial equipment, business contacts, marketing etc. The different websites\n",
    "# which belong to the specific category displays information in different formats. Even with a single website you may not be\n",
    "# able to see all the data at once. The data may be spanned across multiple pages (like google search results, known as\n",
    "# pagination or paginated lists) under various sections.\n",
    "# Using a Web Scraper you can extract data from multiple websites to a single spreadsheet (or database) so that it becomes\n",
    "# easy for you to analyze (or even visualize) the data.\n",
    "\n",
    "# Academic Research:\n",
    "# Data is an integral part of any research, be it academic, marketing or scientific. A Web Scraper will help you gather \n",
    "# structured data from multiple sources in the Internet with ease.\n",
    "\n",
    "# Training and Testing Data for Machine Learning Projects:\n",
    "# Web Scraping helps you to gather data for testing / training your Machine Learning models. Quality of your machine learning\n",
    "# models depends on the quality of training data used and when the data is not readily available you can employ web scraping\n",
    "# to collect it from various websites.\n",
    "\n",
    "# Sports Betting Odds Analysis:\n",
    "# Web scraping is used to collect betting odds values by various bookmakers from sports betting websites like OddsPortal,\n",
    "# BetExplorer, FlashScore etc.\n",
    "\n",
    "# Other uses:\n",
    "# 1. Scrape hotel/restaurant ratings and reviews from websites like TripAdvisor\n",
    "# 2. Scrape hotel room prices and details from websites like Booking.com and Hotels.com\n",
    "# 3. Scrape tweets related to an account or hashtag from Twitter\n",
    "# 4. Scrape profile data from social networks like Facebook, LinkedIn etc. for tracking online reputation.\n",
    "# 5. Scrape hospital/clinic websites to build a catalog of physicians including their contact details\n",
    "# 6. Scrape images and profile data from Instagram\n",
    "# 7. Crawl forums and communities to extract data from posts and authors\n",
    "# 8. Scrape articles from various article/PR websites\n",
    "# 9. Scrape data from various Government websites, most of which do not provide an easy way to download the data which they \n",
    "#    display."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec4183",
   "metadata": {},
   "source": [
    "# What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395e3992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 Best Python Libraries For Web Scraping. Here are the seven most popular Python libraries for web scraping that every data \n",
    "#   professional must be familiar with.\n",
    "# 1.BeautifulSoup.\n",
    "# 2.Scrapy.\n",
    "# 3.Selenium.\n",
    "# 4.Requests.\n",
    "# 5.Urllib3.\n",
    "# 6.Lxml. \n",
    "# 7.MechanicalSoup.\n",
    "\n",
    "# BeautifulSoup:\n",
    "# Features of BeautifulSoup\n",
    "# 1.BeautifulSoup's excellent support for encoding detection is a valuable feature that can yield better outputs for authentic\n",
    "# HTML sites that do not fully disclose their encoding.\n",
    "# 2.Beautiful Soup is built on well-known Python parsers like lxml and html5lib, enabling us to experiment with various parsing\n",
    "# techniques or trade off speed for flexibility.\n",
    "\n",
    "# Pros of BeautifulSoup:\n",
    "# 1.The library helps in maintaining the code's simplicity and adaptability. You can quickly pick up on these features and\n",
    "# execute web scraping to achieve the ideal data extraction outputs if you are a beginner.\n",
    "# 2.While working on the library, Beautiful Soup offers a strong community to address all web scraping challenges for both new\n",
    "# and experienced developers.\n",
    "# The primary benefit of using Beautiful Soup for developers is that it offers excellent and thorough documentation.\n",
    "\n",
    "# Cons of BeautifulSoup:\n",
    "# 1.The use of proxies is not simple with BeautifulSoup. As a result, using BeautifulSoup to download vast volumes of data from\n",
    "# the same site without having your IP blacklisted or banned is difficult.\n",
    "# 2.BeautifulSoup can't function independently as a parser. It requires you to install dependencies before using it.\n",
    "\n",
    "# Some real-world use cases of BeautifulSoup include:\n",
    "# The Python bug tracker was transferred from Sourceforge to Roundup by the Python developers using Beautiful Soup.\n",
    "# Jiabao Lin's DXY-COVID-19-Crawler implements Beautiful Soup to scrape a Chinese medical website for data on COVID-19, making\n",
    "# it easy for researchers to monitor the virus's transmission.The NOAA's Forecast Applications Branch employsBeautifulSoup in \n",
    "# the TopoGrabber script for downloading high-resolution USGS datasets.\n",
    "\n",
    "# Scrapy:\n",
    "# Features of Scrapy:\n",
    "# 1.Scrapy offers built-in support for identifying and extracting data from XML/HTML files using enhanced CSS selectors, XPath\n",
    "# expressions, and helper methods.\n",
    "# 2.This web crawler provides a Telnet console through which you can connect to a Python terminal inside your Scrapy process to\n",
    "# monitor and debug your crawler.\n",
    "# 3.Scrapy has built-in support for creating feed exports in various file types (JSON, CSV, and XML) and storing them in \n",
    "# multiple backends (FTP, S3, local filesystem).\n",
    "\n",
    "# Pros of Scrapy:\n",
    "# 1.Scrapy's robust support for extensibility lets you add your features using signals and a simple API (middlewares, extensions,\n",
    "# and pipelines).\n",
    "# 2.Scrapy provides an interactive shell terminal that is IPython-aware and allows you to test out CSS and XPath expressions to\n",
    "# scrape data when creating or debugging your spiders.\n",
    "# 3.Scrapy provides strong encoding support and auto-detection feature for dealing with foreign, non-standard, and broken\n",
    "# encoding declarations.\n",
    "\n",
    "# Cons of Scrapy:\n",
    "# 1.Scrapy does not work well with javaScript-based websites.\n",
    "# 2.Various operating systems have different installation techniques for Scrapy.\n",
    "# 3.Python 2.7+ is necessary for Scrapy.\n",
    "\n",
    "# Some real-world use cases of Scrapy include:\n",
    "# 1.Intoli employs Scrapy in offering specialized web scraping solutions for its clients' use in generating leads, powering their\n",
    "# core products, and researching competitors.\n",
    "# 2.Lambert Labs specializes in using Scrapy to collect text, images, and videos, both organized and unstructured, from the\n",
    "# entire internet. It integrates Scrapy and Selenium to crawl dynamic websites written in JavaScript continuously.\n",
    "# 3.Alistek employs Scrapy to update partner-related data in their OpenERP-based back-office system by extracting data from\n",
    "# multiple online and offline data sources.\n",
    "\n",
    "# Requests:\n",
    "# Features of Requests\n",
    "# 1.It supports the restful API and its functionalities (PUT, GET, DELETE, and POST) and offers extensive documentation.\n",
    "# 2.The Requests library supports error handling, including Connection Error, Timeout, TooManyRedirect, \n",
    "#   Response.raise_for_status, etc.\n",
    "# 3.Secure URLs include an SSL certificate as a security measure. When you employ Requests, it validates the SSL certificates\n",
    "#  for the HTTPS URL. In the requests library, SSL Verification is present by default; if the certificate is missing, it issues\n",
    "#  an error.\n",
    "\n",
    "# Pros of Requests\n",
    "# 1.Requests library is the best choice if you just start with web scraping and have access to an API. It is easy to understand\n",
    "# and does not require much practice to master.\n",
    "# 2.Requests also minimizes the need to include query strings in your URLs manually.\n",
    "# 3.It supports authentication modules and handles cookies and sessions with excellent stability.\n",
    "\n",
    "# Cons of Requests:\n",
    "# 1.You should not send sensitive data like the username and password via the library's GET method since they are completely\n",
    "# visible in the URL query string and may exist in the client browser's memory as a visited page.\n",
    "# 2.It cannot handle dynamic websites that comprise mostly JavaScript code or parse HTML.\n",
    "\n",
    "# urllib3:\n",
    "# Features of urllib3:\n",
    "# 1.here are two extra tasks you can perform with HTTP requests. First, you can send data to the server directly by passing data\n",
    "# to it. Next, you can provide additional request details in the HTTP headers you send to the server.\n",
    "# 2.It offers the urllib.error module for urllib.request exception handling. These errors, or exceptions, are either HTTP\n",
    "# Errors (triggers due to HTTP errors like 404 and 403) or URL Errors (which occur when your URL is incorrect or there is a\n",
    "# problem with internet connectivity).\n",
    "\n",
    "# Pros of urllib3:\n",
    "# 1.A PoolManager instance keeps track of connection pooling and thread safety so that you don't have to when you employ it to\n",
    "# submit requests.\n",
    "# 2.Developers can access and parse data from protocols like HTTP and FTP using Urllib, which is an added benefit.\n",
    "\n",
    "# Cons of urllib3:\n",
    "# 1.There aren't many features in the urllib library.\n",
    "# 2.It can appear to be a little more challenging than the Requests library.\n",
    "\n",
    "# Compare: BeautifulSoup Vs  Scrapy Vs Selenium \n",
    "# Performance:\n",
    "\n",
    "# BeautifulSoup:Although BeautifulSoup is slow, multithreading can speed it up.\n",
    "\n",
    "# Scrapy:Scrapy is faster and more efficient due to the built-in support for creating feed exports in various formats and\n",
    "# asynchronous request execution.\n",
    "\n",
    "# Selenium:Selenium is efficient; however, the process is slow with large volumes of data. It waits for the entire page to\n",
    "# load while client-side technologies like JavaScript load first.\n",
    "\n",
    "# Ease of Use:\n",
    "\n",
    "# BeautifulSoup:\n",
    "# Beautiful Soup is the ideal place to start for a beginner exploring hands-on web scraping for the first time. Although \n",
    "# simpler, Beautiful Soup can only effectively interact with less complex pages. With only a few lines of code, a user can\n",
    "# start utilizing Beautiful Soup to find all the links on a web page and scrape multiple websites.\n",
    "\n",
    "# Scrapy:\n",
    "# Although Scrapy is a popular web scraping framework, it is significantly more complex than BeautifulSoup and Selenium.\n",
    "# It is not beginner-friendly and has a difficult learning curve.\n",
    "\n",
    "# Selenium:\n",
    "# Selenium can be a complete toolkit for web automation that simulates mouse clicks and forms input. Due to this capability,\n",
    "# the learning curve is more complex for developers.\n",
    "\n",
    "# Extensibility:\n",
    "# BeautifulSoup:\n",
    "# When processing minor projects with low complexity, BeautifulSoup is the best library. This is because it employs simple\n",
    "# and extensible codes. BeautifulSoup is the best library for you if you are a beginner and want to perform web scraping \n",
    "# efficiently.\n",
    "\n",
    "# Scrapy:\n",
    "# Scrapy might be preferable for more complex, large-scale projects because it allows adding custom features and the quick \n",
    "# and flexible creation of pipelines.\n",
    "\n",
    "# Selenium:\n",
    "# Selenium is also quite beneficial, especially with websites that rely heavily on java for various functionalities. However,\n",
    "# selenium users should keep their data volumes within a limit.\n",
    "\n",
    "# Ecosystem:\n",
    "\n",
    "# BeautifulSoup:\n",
    "# BeautifulSoup has a good ecosystem but comes with a lot of dependencies on it.\n",
    "\n",
    "# Scrapy:\n",
    "# You can use proxies with Scrapy to automate your web scraping tasks, and it has a robust ecosystem. Since you can send several\n",
    "# requests at once, it's more suitable for use when you manage complex tasks.\n",
    "\n",
    "# Selenium:\n",
    "# Selenium also has a robust ecosystem but does not make it simple to employ proxies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f04b8",
   "metadata": {},
   "source": [
    "#  What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6e11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautiful Soup is a Python library that makes it easy to scrape information from web pages. It sits atop an HTML or XML\n",
    "# parser and provides Pythonic idioms for iterating, searching, and modifying the parse tree.\n",
    "\n",
    "# Uses of Beautiful Soup:\n",
    "# The Beautiful Soup library helps with isolating titles and links from webpages. It can extract all of the text from HTML\n",
    "# tags, and alter the HTML in the document with which we’re working. Use to Extracting data, Filtering Value, Navigating Data.\n",
    "\n",
    "# Features of Beautiful Soup\n",
    "# Some key features that make beautiful soup unique are:\n",
    "# 1.Beautiful Soup provides a few simple methods and Pythonic idioms for navigating, searching, and modifying a parse tree.\n",
    "# 2.Beautiful Soup automatically converts incoming documents to Unicode and outgoing documents to UTF-8.\n",
    "# 3.Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, which allows us to try out different parsing\n",
    "#   strategies or trade speed for flexibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b71a1",
   "metadata": {},
   "source": [
    "# Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54ba9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in \n",
    "# a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports\n",
    "# the Flask class and the render_template method from the flask library.\n",
    "# It helps to automate web sraping project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1eac5e",
   "metadata": {},
   "source": [
    "# Write the names of AWS services used in this project. Also, explain the use of each service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50c749ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Pipeline:\n",
    "# AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release\n",
    "# your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates\n",
    "# the steps required to release your software changes continuously.\n",
    "#                                                         Valid CodePipeline action types are source , build , test , deploy, \n",
    "# approval and invoke .   \n",
    "\n",
    "# Beanstack: \n",
    "# Elastic Beanstalk is a service for deploying and scaling web applications and services. Upload your code and Elastic Beanstalk\n",
    "# automatically handles the deployment—from capacity provisioning, load balancing, and auto scaling to application health\n",
    "# monitoring.\n",
    "# AWS Elastic Beanstalk is a fully managed service that makes it easy for developers to deploy, run, and scale web applications\n",
    "# and services. It is a Platform as a Service (PaaS) offered by Amazon Web Services (AWS).\n",
    "\n",
    "\n",
    "# Which are features of Elastic Beanstalk?\n",
    "# 1.Wide Selection of Application Platforms. Wide Selection of Application Platforms.\n",
    "# 2.Variety of Application Deployment Options. Variety of Application Deployment Options.\n",
    "# 3.Monitoring.\n",
    "# 4.Management and Updates.\n",
    "# 5.Scaling.\n",
    "# 6.Customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36898527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
